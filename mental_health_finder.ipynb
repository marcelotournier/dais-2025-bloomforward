{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d27f70c7-9635-4ae3-9148-36bf0342e9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq langchain_core langchain_databricks langchain_community\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78e8899-7a2a-4c63-886f-f96ebbc7b417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_databricks import ChatDatabricks\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f8e8d0-5f19-40bf-8c79-e066a848f02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# configure workspace tokens\n",
    "w = WorkspaceClient()\n",
    "os.environ[\"DATABRICKS_HOST\"] = w.config.host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = w.tokens.create(comment=\"for model serving\", lifetime_seconds=1200).token_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "800528e7-e472-4403-972e-eb3e659ce718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"databricks-llama-4-maverick\"\n",
    "# \"databricks-meta-llama-3-1-405b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ce47e1-ac47-4c6c-a0ff-16ed3d254f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save data to a csv locally:\n",
    "df = spark.sql(\"SELECT * FROM `dais-hackathon-2025`.bright_initiative.google_maps_businesses where category = 'Psychotherapist' AND country = 'US'\").toPandas().to_csv(\"data.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb8e92b-0b20-4b11-84c5-4355c2c2ff94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatDatabricks(endpoint=MODEL)\n",
    "\n",
    "def format_context(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Converts the DataFrame into a JSON string to ensure all data is passed\n",
    "    to the model without truncation. JSON is also a great format for structured data\n",
    "    like you have in 'description_by_sections'.\n",
    "    \"\"\"\n",
    "    return df.to_json(orient='records', indent=2)\n",
    "\n",
    "def find_accessible_psychotherapists(location: str) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Queries the Bright Initiative google maps dataset for psychotherapists on a specific location.\n",
    "  \"\"\"\n",
    "  query = pd.read_csv(\"data.csv\")\n",
    "  query = query[query.address.fillna(\"\").str.lower().str.contains(location.lower())]\n",
    "  \n",
    "  return format_context(query)\n",
    "\n",
    "# Define the prompt template for the LLM\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  You are a helpful assistant with the purpose of finding helpful mental health resources. Your goal is to summarize potential psychoterapist venues a user can get in touch.\n",
    "\n",
    "  Summarize the information the user can use to get in touch with.\n",
    "\n",
    "  If the person types the name of a city or location, you should only show the closest locations to the city or place the user sent\n",
    "\n",
    "  Here is the JSON data:\n",
    "  {context}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatDatabricks(endpoint=MODEL)\n",
    "\n",
    "# This is our simple \"agentic\" chain\n",
    "chain = (\n",
    "    find_accessible_psychotherapists\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Let's run the chain for Chicago!\n",
    "result = chain.invoke(\"Chicago\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f14cba6e-83c0-4e73-89c0-b03b50a2c1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Risk Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a82a4a2a-e7bc-4b07-ae82-23a98c086cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3af7edd-bb9f-47b0-b139-06a033fae59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RiskLevel(Enum):\n",
    "    \"\"\"Risk levels for mental health assessment\"\"\"\n",
    "    CRITICAL = \"critical\"  # Immediate danger, explicit suicidal intent\n",
    "    HIGH     = \"high\"      # Suicidal ideation present, planning elements\n",
    "    MODERATE = \"moderate\"  # Depression, hopelessness, passive ideation\n",
    "    LOW      = \"low\"       # General distress, no immediate concern\n",
    "    MINIMAL  = \"minimal\"   # No significant risk indicators\n",
    "\n",
    "class RiskDetector:\n",
    "    \"\"\"\n",
    "    Risk detection system using LLM and rule-based approaches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        databricks_endpoint: str = None,\n",
    "        model_name: str = \"databricks-llama-4-maverick\",\n",
    "        enable_llm: bool = True,\n",
    "        fallback_models: List[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the risk detector for Databricks environment\n",
    "        \n",
    "        Args:\n",
    "            databricks_endpoint: The Databricks serving endpoint base URL\n",
    "            model_name: The model to use for inference\n",
    "            enable_llm: Whether to use LLM for enhanced detection\n",
    "            fallback_models: List of alternative models to try if primary fails\n",
    "        \"\"\"\n",
    "        # Grab Databricks token if running in notebook, else placeholder\n",
    "        try:\n",
    "            self.DATABRICKS_TOKEN = (\n",
    "                dbutils.notebook.entry_point.getDbutils()\n",
    "                .notebook().getContext().apiToken().get()\n",
    "            )\n",
    "        except:\n",
    "            self.DATABRICKS_TOKEN = \"YOUR_TOKEN_HERE\"\n",
    "            logger.warning(\"Could not get Databricks token automatically. Using placeholder.\")\n",
    "        \n",
    "        self.endpoint       = databricks_endpoint or \"https://dbc-9615609d-a312.cloud.databricks.com/serving-endpoints\"\n",
    "        self.model_name     = model_name\n",
    "        self.enable_llm     = enable_llm\n",
    "        self.fallback_models = fallback_models or [\n",
    "            \"databricks-meta-llama-3-1-8b-instruct\",\n",
    "            \"databricks-mistral-7b-instruct\"\n",
    "        ]\n",
    "        \n",
    "        # Initialize OpenAI client pointing at Databricks\n",
    "        self.client = OpenAI(\n",
    "            api_key=self.DATABRICKS_TOKEN,\n",
    "            base_url=self.endpoint\n",
    "        )\n",
    "        \n",
    "        # Rule-based indicator lists\n",
    "        self.critical_indicators = [\n",
    "            \"kill myself\", \"end my life\", \"suicide plan\", \"method to die\",\n",
    "            \"goodbye note\", \"final goodbye\", \"won't be here tomorrow\",\n",
    "            \"end it all tonight\", \"overdose\", \"jump off\", \"hanging myself\",\n",
    "            \"cutting myself\", \"slit my wrists\"\n",
    "        ]\n",
    "        self.high_risk_indicators = [\n",
    "            \"want to die\", \"wish i was dead\", \"better off dead\",\n",
    "            \"thinking about suicide\", \"suicidal thoughts\", \"no reason to live\",\n",
    "            \"can't go on\", \"considering that last option\",\n",
    "            \"don't want to wake up\", \"hope i don't wake up\"\n",
    "        ]\n",
    "        self.moderate_risk_indicators = [\n",
    "            \"hopeless\", \"worthless\", \"burden\", \"don't want to exist\",\n",
    "            \"exhausted from breathing\", \"self destructive\", \"really bad place\",\n",
    "            \"worse than ever\", \"don't feel real\", \"can't take it anymore\",\n",
    "            \"no point\", \"why bother\", \"given up\", \"don't feel like i should exist\",\n",
    "            \"exhausted from just breathing\"\n",
    "        ]\n",
    "    \n",
    "    def detect_risk(self, text: str) -> Tuple[RiskLevel, float, Dict]:\n",
    "        \"\"\"\n",
    "        Detect risk level from user text\n",
    "        \n",
    "        Returns:\n",
    "            (risk_level, confidence_score, detailed_analysis)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # rule-based quick screen\n",
    "            rule_risk = self._rule_based_screening(text)\n",
    "            llm_analysis = {}\n",
    "            \n",
    "            # LLM-based nuance\n",
    "            if self.enable_llm:\n",
    "                llm_analysis = self._llm_risk_assessment(text)\n",
    "            \n",
    "            final_risk, confidence, analysis = self._combine_assessments(\n",
    "                rule_risk, llm_analysis, text\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Risk Assessment - Level: {final_risk.value}, Confidence: {confidence}\")\n",
    "            return final_risk, confidence, analysis\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in risk detection: {e}\")\n",
    "            # fallback to safe setting\n",
    "            return RiskLevel.HIGH, 0.5, {\n",
    "                \"error\": str(e),\n",
    "                \"message\": \"Error in assessment, treating as high risk for safety\"\n",
    "            }\n",
    "    \n",
    "    def _rule_based_screening(self, text: str) -> RiskLevel:\n",
    "        \"\"\"Quick rule-based screening for immediate risk detection\"\"\"\n",
    "        tl = text.lower()\n",
    "        \n",
    "        # critical?\n",
    "        for kw in self.critical_indicators:\n",
    "            if kw in tl:\n",
    "                return RiskLevel.CRITICAL\n",
    "        \n",
    "        # high?\n",
    "        for kw in self.high_risk_indicators:\n",
    "            if kw in tl:\n",
    "                return RiskLevel.HIGH\n",
    "        \n",
    "        # moderate?\n",
    "        count = sum(1 for kw in self.moderate_risk_indicators if kw in tl)\n",
    "        if count >= 3:\n",
    "            return RiskLevel.HIGH\n",
    "        elif count == 2:\n",
    "            return RiskLevel.MODERATE\n",
    "        elif count == 1:\n",
    "            return RiskLevel.LOW\n",
    "        \n",
    "        return RiskLevel.MINIMAL\n",
    "    \n",
    "    def _llm_risk_assessment(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Use LLM for sophisticated risk assessment with fallback models\n",
    "        \"\"\"\n",
    "        system_prompt = \"\"\"\n",
    "You are a mental health risk assessment specialist. Analyze text\n",
    "for suicide risk and mental health crisis indicators. Respond with JSON\n",
    "only, following this scheme:\n",
    "\n",
    "{\n",
    "  \"risk_level\": \"critical/high/moderate/low/minimal\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"risk_factors\": [...],\n",
    "  \"protective_factors\": [...],\n",
    "  \"immediate_danger\": true/false,\n",
    "  \"reasoning\": \"brief explanation\"\n",
    "}\n",
    "Always err on the side of caution.\n",
    "\"\"\".strip()\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "Analyze the following text for suicide risk indicators:\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Respond in the exact JSON format.\n",
    "\"\"\".strip()\n",
    "        \n",
    "        for model in [self.model_name] + self.fallback_models:\n",
    "            try:\n",
    "                resp = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\":\"system\", \"content\": system_prompt},\n",
    "                        {\"role\":\"user\",   \"content\": user_prompt}\n",
    "                    ],\n",
    "                    max_tokens=500,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                content = resp.choices[0].message.content\n",
    "                match = re.search(r'(\\{.*?\\})', content, re.DOTALL)\n",
    "                if match:\n",
    "                    return json.loads(match.group(1))\n",
    "                else:\n",
    "                    logger.warning(f\"Could not parse JSON from model {model}\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model {model} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.warning(\"All LLM models failed; falling back to rule-based only.\")\n",
    "        return {}\n",
    "    \n",
    "    def _combine_assessments(\n",
    "        self,\n",
    "        rule_risk: RiskLevel,\n",
    "        llm_analysis: Dict,\n",
    "        original_text: str\n",
    "    ) -> Tuple[RiskLevel, float, Dict]:\n",
    "        \"\"\"\n",
    "        Combine rule-based and LLM assessments\n",
    "        \"\"\"\n",
    "        final_risk = rule_risk\n",
    "        confidence = 0.7\n",
    "        \n",
    "        if llm_analysis:\n",
    "            llm_str = llm_analysis.get(\"risk_level\", \"\").lower()\n",
    "            llm_conf = llm_analysis.get(\"confidence\", 0.5)\n",
    "            map_int = lambda lvl: {\n",
    "                \"minimal\": 1, \"low\": 2,\n",
    "                \"moderate\": 3, \"high\": 4, \"critical\": 5\n",
    "            }.get(lvl, 3)\n",
    "            llm_risk = RiskLevel(llm_str) if llm_str in RiskLevel._value2member_map_ else RiskLevel.MODERATE\n",
    "            \n",
    "            # pick higher risk\n",
    "            if map_int(llm_risk.value) > map_int(rule_risk.value):\n",
    "                final_risk = llm_risk\n",
    "                confidence = llm_conf\n",
    "            else:\n",
    "                confidence = (confidence + llm_conf) / 2\n",
    "        \n",
    "        analysis = {\n",
    "            \"risk_level\": final_risk.value,\n",
    "            \"confidence\": confidence,\n",
    "            \"rule_based_risk\": rule_risk.value,\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"text_length\": len(original_text),\n",
    "            \"assessment_method\": \"hybrid\"\n",
    "        }\n",
    "        \n",
    "        if llm_analysis:\n",
    "            analysis.update({\n",
    "                \"llm_risk\":          llm_analysis.get(\"risk_level\"),\n",
    "                \"risk_factors\":      llm_analysis.get(\"risk_factors\", []),\n",
    "                \"protective_factors\":llm_analysis.get(\"protective_factors\", []),\n",
    "                \"immediate_danger\":  llm_analysis.get(\"immediate_danger\", False),\n",
    "                \"reasoning\":         llm_analysis.get(\"reasoning\", \"\")\n",
    "            })\n",
    "        \n",
    "        return final_risk, confidence, analysis\n",
    "    \n",
    "    def assess_from_unity_catalog(\n",
    "        self,\n",
    "        table_path: str,\n",
    "        text_column: str = \"text\",\n",
    "        limit: int = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Read rows from Unity Catalog table, assess risk for each, and return results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sql = f\"SELECT * FROM {table_path}\"\n",
    "            if limit:\n",
    "                sql += f\" LIMIT {limit}\"\n",
    "            df = spark.sql(sql)\n",
    "            \n",
    "            results = []\n",
    "            for row in df.collect():\n",
    "                idx = row[\"index_column\"]\n",
    "                txt = row[text_column]\n",
    "                cls = row[\"class\"]\n",
    "                src = row[\"source\"]\n",
    "                \n",
    "                rl, conf, analysis = self.detect_risk(txt)\n",
    "                \n",
    "                results.append({\n",
    "                    \"index_column\": idx,\n",
    "                    \"original_text\": txt,\n",
    "                    \"class\": cls,\n",
    "                    \"source\": src,\n",
    "                    \"risk_assessment\": {\n",
    "                        \"risk_level\": rl.value,\n",
    "                        \"confidence\": conf,\n",
    "                        \"analysis\": analysis\n",
    "                    }\n",
    "                })\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading from Unity Catalog: {e}\")\n",
    "            return []\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mental_health_finder",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
